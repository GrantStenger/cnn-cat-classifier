{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cats vs Dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workshop you will be building a Convolutional neural network to classify cats vs dogs. You will need to be familiar with the theory of CNNs. Visit our lesson [here](http://caisplusplus.usc.edu/blog/curriculum/lesson7) for more info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.ndimage import imread\n",
    "import cv2\n",
    "import sklearn.utils\n",
    "\n",
    "DATA_PATH = './data/'\n",
    "TEST_PERCENT = .15\n",
    "SELECT_SUBSET_PERCENT = 1.0\n",
    "\n",
    "# The cat and dog images are of variable size we have to resize them to all the same size.\n",
    "RESIZE_WIDTH=64\n",
    "RESIZE_HEIGHT=64\n",
    "# We are setting this to be 5 epochs for fast training times. \n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "Load the train and test data sets. Do not modify this code at all. Make sure that your data for cats and dogs images is in ``./data``. You can find that data at https://www.kaggle.com/c/dogs-vs-cats/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to load 25000 files\n",
      "Have loaded 1000 samples\n",
      "Have loaded 2000 samples\n",
      "Have loaded 3000 samples\n",
      "Have loaded 4000 samples\n",
      "Have loaded 5000 samples\n",
      "Have loaded 6000 samples\n",
      "Have loaded 7000 samples\n",
      "Have loaded 8000 samples\n",
      "Have loaded 9000 samples\n",
      "Have loaded 10000 samples\n",
      "Have loaded 11000 samples\n",
      "Have loaded 12000 samples\n",
      "Have loaded 13000 samples\n",
      "Have loaded 14000 samples\n",
      "Have loaded 15000 samples\n",
      "Have loaded 16000 samples\n",
      "Have loaded 17000 samples\n",
      "Have loaded 18000 samples\n",
      "Have loaded 19000 samples\n",
      "Have loaded 20000 samples\n",
      "Have loaded 21000 samples\n",
      "Have loaded 22000 samples\n",
      "Have loaded 23000 samples\n",
      "Have loaded 24000 samples\n",
      "Train set has dimensionality (21250, 64, 64, 3)\n",
      "Test set has dimensionality (3750, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# Let's get started by loading the data.\n",
    "# Make sure you have the data downloaded to ./data\n",
    "# To download the data go to https://www.kaggle.com/c/dogs-vs-cats/data and download train.zip\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "files = os.listdir(DATA_PATH)\n",
    "# Shuffle so we are selecting about an equal number of dog and cat images.\n",
    "shuffled_files = sklearn.utils.shuffle(files)\n",
    "select_count = int(len(shuffled_files) * SELECT_SUBSET_PERCENT)\n",
    "\n",
    "print('Going to load %i files' % select_count)\n",
    "\n",
    "subset_files_select = shuffled_files[:select_count]\n",
    "\n",
    "DISPLAY_COUNT = 1000\n",
    "\n",
    "for i, input_file in enumerate(subset_files_select):\n",
    "    if i % DISPLAY_COUNT == 0 and i != 0:\n",
    "        print('Have loaded %i samples' % i)\n",
    "        \n",
    "    img = imread(DATA_PATH + input_file)\n",
    "    # Resize the images to be the same size.\n",
    "    img = cv2.resize(img, (RESIZE_WIDTH, RESIZE_HEIGHT), interpolation=cv2.INTER_CUBIC)\n",
    "    X.append(img)\n",
    "    if 'cat' == input_file.split('.')[0]:\n",
    "        Y.append(0.0)\n",
    "    else:\n",
    "        Y.append(1.0)\n",
    "        \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "test_size = int(len(X) * TEST_PERCENT)\n",
    "\n",
    "test_X = X[:test_size]\n",
    "test_Y = Y[:test_size]\n",
    "train_X = X[test_size:]\n",
    "train_Y = Y[test_size:]\n",
    "\n",
    "print('Train set has dimensionality %s' % str(train_X.shape))\n",
    "print('Test set has dimensionality %s' % str(test_X.shape))\n",
    "\n",
    "# Apply some normalization here.\n",
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "train_X /= 255\n",
    "test_X /= 255\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "While not necessary for this problem you can go ahead and try some preprocessing steps to try to get higher accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform any data preprocessing steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the network\n",
    "Here are some useful resources to help with defining a powerful network.\n",
    "- Convolution layers (use the 2D convolution) https://keras.io/layers/convolutional/\n",
    "- Batch norm layer https://keras.io/layers/normalization/\n",
    "- Layer initializers https://keras.io/initializers/\n",
    "- Dense layer https://keras.io/layers/core/#dense\n",
    "- Activation functions https://keras.io/layers/core/#activation\n",
    "- Regulizers: \n",
    "    - https://keras.io/layers/core/#dropout\n",
    "    - https://keras.io/regularizers/\n",
    "    - https://keras.io/callbacks/#earlystopping\n",
    "    - https://keras.io/constraints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary layers.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Activation, MaxPooling2D, Dropout, Flatten, BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Define the network\n",
    "model.add(Conv2D(filters=16, kernel_size=(4,4), input_shape=(64,64,3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=32, kernel_size=(4,4)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=64, kernel_size=(4,4)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "# Define your loss and your objective\n",
    "optimizer = optimizers.RMSprop()\n",
    "loss = \"binary_crossentropy\"\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Time\n",
    "Train the network. Be on the lookout for the validation loss and accuracy. Don't change any of the parameters here except for the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17000 samples, validate on 4250 samples\n",
      "Epoch 1/5\n",
      "17000/17000 [==============================] - 108s - loss: 0.6434 - acc: 0.6416 - val_loss: 0.5902 - val_acc: 0.6729\n",
      "Epoch 2/5\n",
      "17000/17000 [==============================] - 108s - loss: 0.5171 - acc: 0.7518 - val_loss: 0.5690 - val_acc: 0.6969\n",
      "Epoch 3/5\n",
      "17000/17000 [==============================] - 105s - loss: 0.4518 - acc: 0.7901 - val_loss: 0.5957 - val_acc: 0.7532\n",
      "Epoch 4/5\n",
      "17000/17000 [==============================] - 105s - loss: 0.4039 - acc: 0.8230 - val_loss: 0.6732 - val_acc: 0.6835\n",
      "Epoch 5/5\n",
      "17000/17000 [==============================] - 104s - loss: 0.3634 - acc: 0.8393 - val_loss: 0.4926 - val_acc: 0.8002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18f7c5208>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "model.fit(train_X, train_Y, batch_size=batch_size, epochs=EPOCHS, validation_split=0.2, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Time\n",
    "Now it's time to actually test the network. \n",
    "\n",
    "Get above **65%**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3744/3750 [============================>.] - ETA: 0s\n",
      "Got 82.00% accuracy\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_X, test_Y, batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('')\n",
    "print('Got %.2f%% accuracy' % (acc * 100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:caispp]",
   "language": "python",
   "name": "conda-env-caispp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
